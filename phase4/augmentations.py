import torch
import numpy as np
from torchvision import transforms
from PIL import Image, ImageEnhance, ImageOps
import random
import os
import base64
from openai import OpenAI  # éœ€è¦å®‰è£…: pip install openai

# å¦‚æœæ²¡æœ‰å®‰è£… nlpaugï¼Œæä¾›ä¸€ä¸ªç©ºçš„å ä½ç¬¦ä»¥é˜²æŠ¥é”™
try:
    import nlpaug.augmenter.word as naw
except ImportError:
    print("Warning: 'nlpaug' not found. Text augmentation will be disabled.")
    naw = None

class DataAugmenter:
    """
    åŸºç¡€æ•°æ®å¢å¼ºç±» (åŒä¹‰è¯æ›¿æ¢ + å›¾åƒæ‰°åŠ¨)
    """
    def __init__(self):
        # 1. æ–‡æœ¬åŸºç¡€å¢å¼º (åŒä¹‰è¯æ›¿æ¢)
        self.text_aug = None
        if naw is not None:
            try:
                # å°è¯•åˆå§‹åŒ– wordnetï¼Œå¦‚æœä¸‹è½½å¤±è´¥åˆ™è·³è¿‡
                self.text_aug = naw.SynonymAug(aug_src='wordnet')
            except Exception as e:
                print(f"Text Aug init failed: {e}")

        # 2. å›¾åƒåŸºç¡€å¢å¼º (å¼±å¢å¼ºï¼Œé˜²æ­¢ç ´åè¯­ä¹‰)
        self.basic_img_transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomRotation(degrees=10),
        ])

    def basic_augment(self, text, image):
        """åŸºç¡€å¢å¼ºï¼šéšæœºå¯¹æ–‡æœ¬æˆ–å›¾åƒè¿›è¡Œæ‰°åŠ¨"""
        aug_text = text
        aug_image = image
        
        # 50% æ¦‚ç‡å¢å¼ºæ–‡æœ¬
        if self.text_aug and random.random() > 0.5:
            try:
                # nlpaug è¿”å›çš„æ˜¯ list
                res = self.text_aug.augment(text)
                if isinstance(res, list):
                    aug_text = res[0]
                else:
                    aug_text = res
            except:
                pass
                
        # 50% æ¦‚ç‡å¢å¼ºå›¾åƒ
        if random.random() > 0.5:
            aug_image = self.basic_img_transform(image)
            
        return aug_text, aug_image

class VLMAugmenter:
    """
    [Phase 4 æ ¸å¿ƒ] VLM API è°ƒç”¨æ¥å£
    ä½¿ç”¨ ECNU API ä¸º Bad Case ç”Ÿæˆå›¾ç‰‡æè¿°
    """
    def __init__(self, 
                 api_key="sk-ee3a6bcdb0e442be9259d84599b03675", 
                 base_url="https://chat.ecnu.edu.cn/open/api/v1", 
                 model="ecnu-vl"):
        
        self.model = model
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (é€‚é… ECNU æ¥å£)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        print(f"âœ… VLM Augmenter initialized with model: {self.model}")

    def encode_image(self, image_path):
        """å°†æœ¬åœ°å›¾ç‰‡è½¬ä¸º Base64 æ ¼å¼"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def generate_caption(self, image_path):
        """
        è°ƒç”¨ ECNU å¤šæ¨¡æ€å¤§æ¨¡å‹ç”Ÿæˆå›¾ç‰‡æè¿°
        """
        try:
            # 1. ç¼–ç å›¾ç‰‡
            base64_image = self.encode_image(image_path)
            
            # 2. å‘é€è¯·æ±‚
            # Prompt è®¾è®¡é‡ç‚¹ï¼šè¦æ±‚å®¢è§‚æè¿°ï¼Œé¿å…ä¸»è§‚æƒ…æ„Ÿ (Label Leakage)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text", 
                                "text": "Please provide a detailed and objective description of the visual content of this image. Do not analyze the sentiment or emotion."
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=300  # é™åˆ¶è¾“å‡ºé•¿åº¦
            )
            
            caption = response.choices[0].message.content
            return caption.strip()
            
        except Exception as e:
            print(f"âŒ API Error processing {image_path}: {e}")
            # å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œåç»­é€»è¾‘ä¼šè·³è¿‡æ­¤æ ·æœ¬
            return ""

    def augment_dataset(self, bad_case_guids, data_dir, output_file):
        """
        æ ¸å¿ƒæµç¨‹ï¼šéå† Bad Cases -> ç”Ÿæˆæè¿° -> ä¿å­˜æ–°æ ·æœ¬ç´¢å¼•
        """
        print(f"ğŸš€ Starting VLM Augmentation for {len(bad_case_guids)} bad cases...")
        
        # 1. è¯»å–åŸå§‹ç´¢å¼•æ–‡ä»¶ (train_cleaned.txt)
        # å‡è®¾åœ¨ä¸Šä¸¤çº§ç›®å½• ../dataset/train_cleaned.txt
        original_index_path = os.path.join(data_dir, "..", "train_cleaned.txt")
        if not os.path.exists(original_index_path):
            print(f"Error: Original index file not found at {original_index_path}")
            return

        with open(original_index_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        header = lines[0]
        # å»ºç«‹ guid -> raw_line æ˜ å°„
        data_map = {}
        for line in lines[1:]:
            parts = line.strip().split(',')
            if len(parts) >= 2:
                data_map[parts[0]] = line.strip()
        
        new_sample_lines = []
        success_count = 0
        
        # 2. éå† Bad Cases
        for i, guid in enumerate(bad_case_guids):
            if guid not in data_map: continue
            
            # æ‰“å°è¿›åº¦
            if i % 5 == 0:
                print(f"   Processing {i}/{len(bad_case_guids)}: {guid} ...")

            original_line = data_map[guid]
            _, label = original_line.split(',')
            
            # å¯»æ‰¾å›¾ç‰‡ (jpg æˆ– png)
            img_path = os.path.join(data_dir, f"{guid}.jpg")
            if not os.path.exists(img_path):
                img_path = os.path.join(data_dir, f"{guid}.png")
            
            if not os.path.exists(img_path):
                print(f"   Skip {guid}: Image not found.")
                continue
            
            # === è°ƒç”¨ API ===
            new_text = self.generate_caption(img_path)
            
            if not new_text:
                continue # API å¤±è´¥åˆ™è·³è¿‡
            
            # === ä¿å­˜æ–°æ ·æœ¬ ===
            # æ–°çš„ guid å‘½åä¸º {guid}_vlm
            new_guid = f"{guid}_vlm"
            
            # å†™å…¥æ–°çš„æ–‡æœ¬æ–‡ä»¶
            new_txt_path = os.path.join(data_dir, f"{new_guid}.txt")
            with open(new_txt_path, 'w', encoding='utf-8') as f:
                f.write(new_text)
            
            # è®°å½•åˆ°ç´¢å¼•åˆ—è¡¨ (å›¾ç‰‡å¤ç”¨é€»è¾‘ç”± Dataset ç±»å¤„ç†)
            new_sample_lines.append(f"{new_guid},{label}\n")
            success_count += 1
            
        # 3. åˆå¹¶ä¿å­˜ï¼šåŸå§‹æ•°æ® + æ–°å¢æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥è¡¨å¤´
            f.write(header)
            # å†™å…¥åŸå§‹æ•°æ®
            f.writelines(lines[1:])
            # å†™å…¥æ–°å¢æ•°æ®
            f.writelines(new_sample_lines)
            
        print(f"\nâœ… Augmentation Complete!")
        print(f"   - Original samples: {len(lines)-1}")
        print(f"   - VLM Augmented samples: {success_count}")
        print(f"   - Total training samples: {len(lines)-1 + success_count}")
        print(f"   - Saved to: {output_file}")